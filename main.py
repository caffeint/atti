import streamlit as st
from utils import print_messages, StreamHandler, embed_file
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts.base import BasePromptTemplate
from langchain_core.prompts import loading
from langchain_core.runnables import RunnablePassthrough
import yaml

# from langchain_core.runnables import ConfigurableFieldSpec
# from langchain_community.chat_models import ChatOllama
from dotenv import load_dotenv
import os

global retriever

load_dotenv()
st.set_page_config(page_title="ì•„ë  GPT", page_icon="ğŸ¢")
st.title("ğŸ¢ ë˜‘ë ")

# API KEY ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")


if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” store ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()
# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ


with st.sidebar:
    session_id = st.text_input("Session ID", value="chat_1")
    uploaded_file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "pptx", "docx"],
    )

    clear_btn = st.button("ëŒ€í™”ê¸°ë¡ ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state["messages"] = []
        st.rerun()

if uploaded_file:
    # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„± (ì‘ì—…ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ì˜ˆì •...)
    retriever = embed_file(uploaded_file)
    st.session_state["retriever"] = retriever
    # chain = create_chain(retriever, model_name="gpt-4o-mini")
    # st.session_state["chain"] = chain

print_messages()


def load_prompt(file_path, encoding="utf8") -> BasePromptTemplate:
    with open(file_path, "r", encoding=encoding) as f:
        config = yaml.safe_load(f)

    return loading.load_prompt_from_config(config)


# # ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
# def get_session_history(session_ids: str, chat_hist_id: str) -> BaseChatMessageHistory:
#     if (session_ids, chat_hist_id) not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
#         # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
#         st.session_state["store"][(session_ids, chat_hist_id)] = ChatMessageHistory()
#     return st.session_state["store"][(session_ids, chat_hist_id)]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜


# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜


if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))

    # AIì˜ ë‹µë³€
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        # íŒŒì¸íŠœë‹ LLMì„ ì‚¬ìš©í•˜ì—¬ AIì˜ ë‹µë³€ì„ ìƒì„±

        # 1. ëª¨ë¸ ìƒì„±
        # 1-1 openai
        fine_llm = ChatOpenAI(
            streaming=True,
            model="gpt-4o-mini",
            temperature=0,
        )
        # 1-2 ollama model load
        # llm = ChatOllama(model="EEVE-Korean-test:latest")
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        fine_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    # "ë„ˆëŠ” ê²½ê³„ì„  ì§€ëŠ¥ì¸ì„ ë„ì™€ì£¼ëŠ” ì¹œí•œ ì¹œêµ¬ì´ì ì„ ìƒë‹˜ ì—­í• ì„ í•´ì¤˜. \
                    # ë„ˆì˜ ëª©í‘œëŠ” ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²½ê³„ì„  ì§€ëŠ¥ì¸ë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…ì„ í•´ì£¼ëŠ” ê²ƒì´ ëª©í‘œì•¼. \
                    # ë‹µë³€ì€ ê¼¼ê¼¼í•˜ê²Œ ì ˆì°¨ì— ë§ê²Œ ìƒì„¸íˆ ì•Œë ¤ì¤˜. \
                    # ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì¤˜ \
                    # ê·¸ë¦¬ê³  ë„ˆê°€ ë‹µë³€í•œ ë‚´ìš©ì— ëŒ€í•œ ì¶œì²˜ ë‚¨ê²¨ì¤˜"
                    # "ë‹¹ì‹ ì€ ì„¸ìƒì˜ ëª¨ë“  ì¢…ë¥˜ì˜ ì§€ì‹ì— í†µë‹¬í•œ ì²œì¬ í•™ìì´ë©°, ê²½ê³„ì„  ì§€ëŠ¥ì¸ë“¤ì„ ê°€ë¥´ì¹˜ëŠ” ì„ ìƒì…ë‹ˆë‹¤. ê²½ê²Œì„  ì§€ëŠ¥ì¸ë“¤ë„ ì´í•´í•˜ê¸°ì‰½ë„ë¡ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ CoT ê¸°ë²•ì„ ì ìš©í•˜ì—¬, ì°¨ê·¼ì°¨ê·¼ ìƒê°í•´ë³´ê³  ì˜¤ë¥˜ëŠ” ì—†ëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ì—¬ ë‹µë³€í•˜ì„¸ì˜¤. ë‹µë³€ì€ ê²½ê³„ì„  ì§€ëŠ¥ì¸ë“¤ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–´íœ˜ë§Œì„ ì‚¬ìš©í•˜ë©° ì§§ê²Œ í•œêµ­ì–´ë¡œ í•´ì•¼í•©ë‹ˆë‹¤. í•„ìš”í•˜ë©´ ì‰¬ìš´ ì˜ˆì‹œë¥¼ ë“¤ì–´ì£¼ì„¸ìš”. ì•ˆì „ì— ê´€ë ¨í•´ì„œ ìœ ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ ê°•ì¡°í•´ë„ ì¢‹ìŠµë‹ˆë‹¤. ë ˆí¼ëŸ°ìŠ¤ê°€ ë˜ëŠ” ë…¼ë¬¸ì´ë‚˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ê³ , ì²¨ë¶€í•˜ëŠ” í–‰ë™ì„ ì ê·¹ ê¶Œì¥í•©ë‹ˆë‹¤."
                    "You are the world's leading expert on borderline intelligence learning, and a teacher who teaches borderline intelligentsia. \
                    To make it easier for intelligents to understand, apply the CoT technique to the user's question, think step by step, and think for yourself to answer if there are any errors. \
                    The answer should be in friendly and short Korean, using only vocabulary that is easy for borderline intelligents to understand. \
                    If necessary, give me an easy example. \
                    If you have any safety precautions, you can emphasize them. \
                    Please refer to the reference paper or link and highly recommend attaching it"
                ),
                # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥
            ]
        )

        fine_chain = fine_prompt | fine_llm

        fine_chain_with_memory = (
            RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
                fine_chain,  # ì‹¤í–‰í•  Runnable ê°ì²´
                get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                input_messages_key="question",  # ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤
                history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
            )
        )
        fine_response = fine_chain_with_memory.invoke(
            {"question": user_input},
            # ì„¸ì…˜ ID ì„¤ì •
            config={"configurable": {"session_id": session_id}},
        )

        # í„°ë¯¸ë„ì— ì‘ë‹µ ë°ì´í„° ì¶œë ¥
        # print(f"{fine_response.content}")

        ###################################
        # GPT LLMì„ ì‚¬ìš©í•˜ì—¬ AIì˜ ë‹µë³€ì„ ìƒì„±

        # 1. ëª¨ë¸ ìƒì„±
        # 1-1 openai
        GPT_llm = ChatOpenAI(
            streaming=True,
            model="gpt-4o-mini",
            callbacks=[stream_handler],
            temperature=0,
        )
        # 1-2 ollama model load
        # llm = ChatOllama(model="EEVE-Korean-test:latest")
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        GPT_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    f"You are the world's leading expert on borderline intelligence learning.\
                Your colleague, who has the ability to match you, answered [{fine_response.content}] to a question from a borderline intelligent. \
                Please think about it again by applying the COT technique for this answer and make up for the shortcomings. \
                Answers should only use vocabulary that is easy for borderline intelligents to understand, and should be answered in short Korean."
                ),
                # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥
            ]
        )

        GPT_chain = GPT_prompt | GPT_llm

        GPT_chain_with_memory = (
            RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
                GPT_chain,  # ì‹¤í–‰í•  Runnable ê°ì²´
                get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                input_messages_key="question",  # ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤
                history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
            )
        )
        GPT_response = GPT_chain_with_memory.invoke(
            {"question": user_input},
            # ì„¸ì…˜ ID ì„¤ì •
            config={"configurable": {"session_id": session_id}},
        )

        # print(f"///////////////\n {GPT_response.content}")

        st.session_state["messages"].append(
            ChatMessage(role="assistant", content=GPT_response.content)
        )
